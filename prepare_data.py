#!/usr/bin/env python3
"""
GRACEé¡¹ç›®æ•°æ®å‡†å¤‡è„šæœ¬
è‡ªåŠ¨ä»Hugging Faceä¸‹è½½å’Œå‡†å¤‡æ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    python prepare_data.py --data-root /root/sj-tmp/dataset/
    python prepare_data.py --download-model  # ä¸‹è½½æ•°æ®é›†
    python prepare_data.py --all  # ä¸‹è½½æ‰€æœ‰æ•°æ®
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from datasets import load_dataset, Dataset
    from datasets.exceptions import DatasetNotFoundError
    import pandas as pd
    import time
    import requests
    from huggingface_hub import HfApi
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·è¿è¡Œ: pip install -r requirements.txt")
    sys.exit(1)

from config.config import Config

# é…ç½®HuggingFaceé•œåƒæº - è§£å†³äº‘è®¡ç®—å¹³å°è¿æ¥é—®é¢˜
HF_MIRRORS = [
    "https://hf-mirror.com",  # å®˜æ–¹é•œåƒ
    "https://huggingface.co",  # åŸå§‹åœ°å€
    "https://hf-mirror.com",  # å¤‡ç”¨é•œåƒ
]

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨é•œåƒæº
os.environ.setdefault("HF_ENDPOINT", HF_MIRRORS[0])
os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")

# é…ç½®ä»£ç†è®¾ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
PROXY_CONFIG = {
    "http_proxy": os.environ.get("HTTP_PROXY", ""),
    "https_proxy": os.environ.get("HTTPS_PROXY", ""),
}

class DataPreparator:
    """æ•°æ®ä¸‹è½½å’Œå‡†å¤‡å™¨ - å¢å¼ºç‰ˆç½‘ç»œå¤„ç†"""
    
    def __init__(self, data_root: str = "/root/sj-tmp/-dataset/"):
        """
        åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨
        
        Args:
            data_root: æ•°æ®é›†å­˜å‚¨æ ¹ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨äº‘è®¡ç®—å¹³å°è·¯å¾„ï¼‰
        """
        self.data_root = Path(data_root)
        self.config = Config(data_root=data_root)
        self.setup_logging()
        self.max_retries = 3
        self.retry_delay = 5
        
        # åˆå§‹åŒ–é•œåƒæºé…ç½®
        self.hf_mirrors = HF_MIRRORS.copy()
        self.current_mirror_index = 0
        self.api = self._init_hf_api_with_mirrors()
        
        # ç¡®ä¿æ•°æ®æ ¹ç›®å½•å­˜åœ¨
        self.data_root.mkdir(parents=True, exist_ok=True)
        
        print(f"æ•°æ®æ ¹ç›®å½•: {self.data_root}")
        print(f"é…ç½®æ•°æ®é›†: {list(self.config.datasets.keys())}")
        print(f"å½“å‰HuggingFaceé•œåƒæº: {self.hf_mirrors[self.current_mirror_index]}")
        print(f"æ•°æ®é›†å°†ä¿å­˜åˆ°: {self.data_root}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = self.config.logs_dir
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"data_preparation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format=self.config.log_format,
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"æ•°æ®å‡†å¤‡æ—¥å¿—: {log_file}")
    
    def _init_hf_api_with_mirrors(self) -> HfApi:
        """åˆå§‹åŒ–å¸¦é•œåƒæºçš„HuggingFace API"""
        try:
            # è®¾ç½®å½“å‰é•œåƒæº
            current_mirror = self.hf_mirrors[self.current_mirror_index]
            os.environ["HF_ENDPOINT"] = current_mirror
            
            # é…ç½®ä»£ç†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if PROXY_CONFIG["http_proxy"] or PROXY_CONFIG["https_proxy"]:
                self.logger.info(f"ä½¿ç”¨ä»£ç†é…ç½®: {PROXY_CONFIG}")
            
            return HfApi()
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–HuggingFace APIå¤±è´¥: {e}")
            return HfApi()
    
    def _switch_to_next_mirror(self) -> bool:
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªé•œåƒæº"""
        if self.current_mirror_index < len(self.hf_mirrors) - 1:
            self.current_mirror_index += 1
            current_mirror = self.hf_mirrors[self.current_mirror_index]
            os.environ["HF_ENDPOINT"] = current_mirror
            self.api = self._init_hf_api_with_mirrors()
            self.logger.info(f"åˆ‡æ¢åˆ°é•œåƒæº: {current_mirror}")
            return True
        else:
            self.logger.error("æ‰€æœ‰é•œåƒæºéƒ½å·²å°è¯•ï¼Œæ— æ³•è¿æ¥")
            return False
    
    def _test_network_connectivity(self) -> bool:
        """æµ‹è¯•ç½‘ç»œè¿æ¥ - æ”¯æŒé•œåƒæºåˆ‡æ¢"""
        for mirror_index in range(len(self.hf_mirrors)):
            current_mirror = self.hf_mirrors[mirror_index]
            try:
                self.logger.info(f"æµ‹è¯•é•œåƒæºè¿æ¥: {current_mirror}")
                response = requests.get(current_mirror, timeout=10)
                if response.status_code == 200:
                    # å¦‚æœå½“å‰ä½¿ç”¨çš„ä¸æ˜¯è¿™ä¸ªå¯ç”¨çš„é•œåƒæºï¼Œåˆ‡æ¢åˆ°å®ƒ
                    if mirror_index != self.current_mirror_index:
                        self.current_mirror_index = mirror_index
                        os.environ["HF_ENDPOINT"] = current_mirror
                        self.api = self._init_hf_api_with_mirrors()
                        self.logger.info(f"åˆ‡æ¢åˆ°å¯ç”¨çš„é•œåƒæº: {current_mirror}")
                    return True
            except Exception as e:
                self.logger.warning(f"é•œåƒæº {current_mirror} è¿æ¥å¤±è´¥: {e}")
                continue
        
        self.logger.error("æ‰€æœ‰é•œåƒæºè¿æ¥æµ‹è¯•å¤±è´¥")
        return False
    
    def _get_available_alternatives(self, dataset_type: str) -> List[str]:
        """
        è·å–å¯ç”¨çš„æ›¿ä»£æ•°æ®é›†
        
        Args:
            dataset_type: æ•°æ®é›†ç±»å‹ ('bigvul', 'reveal', 'devign')
            
        Returns:
            List[str]: æ›¿ä»£æ•°æ®é›†åˆ—è¡¨
        """
        alternatives = {
            "bigvul": [
                "Junwei/MSR",  # ä¸»è¦æ›¿ä»£æ•°æ®é›† - ç»è¿‡éªŒè¯å¯ç”¨
                "FFJSJ/BigVul",
                "microsoft/BigVul-Benchmark"
            ],
            "reveal": [
                "microsoft/CodeXGLUE",  # ç»è¿‡éªŒè¯å¯ç”¨
                "codebert/ReVeal-Extended",
                "claudios/ReVeal-dataset"  # å¤‡ç”¨æ•°æ®æº
            ],
            "devign": [
                "microsoft/Devign-Benchmark", 
                "DetectVul/devign-processed",  # ç»è¿‡éªŒè¯å¯ç”¨
                "codebert/Devign-Filtered"
            ]
        }
        
        return alternatives.get(dataset_type, [])
    
    def _load_dataset_with_retry(self, dataset_path: str, max_retries: int = None) -> Optional[Dict]:
        """å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®é›†åŠ è½½ - æ”¯æŒé•œåƒæºåˆ‡æ¢"""
        if max_retries is None:
            max_retries = self.max_retries
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"å°è¯•åŠ è½½æ•°æ®é›† {dataset_path} (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)")
                self.logger.info(f"å½“å‰é•œåƒæº: {self.hf_mirrors[self.current_mirror_index]}")
                
                dataset = load_dataset(dataset_path)
                self.logger.info(f"æ•°æ®é›† {dataset_path} åŠ è½½æˆåŠŸ")
                return dataset
            except Exception as e:
                self.logger.warning(f"ç¬¬ {attempt + 1} æ¬¡åŠ è½½å¤±è´¥: {e}")
                
                # å¦‚æœè¿˜æœ‰é•œåƒæºå¯ä»¥åˆ‡æ¢ï¼Œå°è¯•åˆ‡æ¢é•œåƒæº
                if self._switch_to_next_mirror():
                    self.logger.info("åˆ‡æ¢é•œåƒæºåç»§ç»­å°è¯•")
                    continue
                
                if attempt < max_retries - 1:
                    wait_time = self.retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"æ•°æ®é›† {dataset_path} åŠ è½½å¤±è´¥ï¼Œå·²å°è¯• {max_retries} æ¬¡")
        
        return None
    
    def download_dataset(self, dataset_name: str) -> bool:
        """
        ä¸‹è½½æŒ‡å®šæ•°æ®é›† - å¢å¼ºç‰ˆç½‘ç»œå¤„ç†ï¼ˆæ”¯æŒé•œåƒæºåˆ‡æ¢ï¼‰
        
        Args:
            dataset_name: æ•°æ®é›†åç§° (bigvul, reveal, devign)
            
        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if dataset_name not in self.config.datasets:
            self.logger.error(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
            return False
        
        dataset_info = self.config.datasets[dataset_name]
        huggingface_url = dataset_info["huggingface_url"]
        
        self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_name} ({huggingface_url})")
        self.logger.info(f"å½“å‰é•œåƒæº: {self.hf_mirrors[self.current_mirror_index]}")
        
        # é¦–å…ˆæ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆä¼šè‡ªåŠ¨æµ‹è¯•å¹¶é€‰æ‹©æœ€ä½³é•œåƒæºï¼‰
        if not self._test_network_connectivity():
            self.logger.error("ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ— æ³•è®¿é—®ä»»ä½•HuggingFaceé•œåƒæº")
            self.logger.info("è§£å†³æ–¹æ¡ˆ:")
            self.logger.info("1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            self.logger.info("2. é…ç½®ä»£ç†æœåŠ¡å™¨ (è®¾ç½®HTTP_PROXY/HTTPS_PROXYç¯å¢ƒå˜é‡)")
            self.logger.info("3. ä½¿ç”¨VPNè¿æ¥")
            self.logger.info("4. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
            self.logger.info("5. ç¨åé‡è¯•")
            return False
        
        # é‡ç½®é•œåƒæºç´¢å¼•ï¼Œç¡®ä¿ä»æœ€ä½³é•œåƒæºå¼€å§‹
        self.current_mirror_index = 0
        
        try:
            # ä»Hugging FaceåŠ è½½æ•°æ®é›†ï¼ˆæ”¯æŒé•œåƒæºåˆ‡æ¢å’Œé‡è¯•ï¼‰
            if dataset_name == "bigvul":
                dataset = self._load_bigvul_dataset()
            elif dataset_name == "reveal":
                dataset = self._load_reveal_dataset()
            elif dataset_name == "devign":
                dataset = self._load_devign_dataset()
            else:
                self.logger.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
                return False
            
            if dataset is None:
                self.logger.error(f"æ•°æ®é›† {dataset_name} ä¸‹è½½å¤±è´¥")
                return False
            
            # ä¿å­˜æ•°æ®é›†
            success = self._save_dataset(dataset, dataset_name)
            
            if success:
                self.logger.info(f"æ•°æ®é›† {dataset_name} ä¸‹è½½å®Œæˆ")
                self.logger.info(f"æœ€ç»ˆä½¿ç”¨çš„é•œåƒæº: {self.hf_mirrors[self.current_mirror_index]}")
                return True
            else:
                self.logger.error(f"æ•°æ®é›† {dataset_name} ä¿å­˜å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ•°æ®é›† {dataset_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _load_bigvul_dataset(self) -> Optional[Dict[str, Dataset]]:
        """åŠ è½½BigVulæ•°æ®é›† - å¢å¼ºç‰ˆç½‘ç»œå¤„ç†"""
        self.logger.info("åŠ è½½ BigVul æ•°æ®é›†...")
        
        # é¦–å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
        if not self._test_network_connectivity():
            self.logger.error("ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ— æ³•è®¿é—® HuggingFace Hub")
            return None
        
        # å°è¯•åŠ è½½ä¸»è¦æ•°æ®é›†
        full_dataset = self._load_dataset_with_retry("bstee615/bigvul")
        if full_dataset is None:
            self.logger.warning("ä¸»è¦æ•°æ®é›† bstee615/bigvul åŠ è½½å¤±è´¥ï¼Œå°è¯•æ›¿ä»£æ•°æ®é›†...")
            
            # å°è¯•æ›¿ä»£æ•°æ®é›† - ä¼˜å…ˆä½¿ç”¨å·²éªŒè¯çš„å¯ç”¨æ•°æ®é›†
            alternatives = self._get_available_alternatives("bigvul")
            for alt_dataset in alternatives:
                self.logger.info(f"å°è¯•æ›¿ä»£æ•°æ®é›†: {alt_dataset}")
                # å¯¹äºJunwei/MSRæ•°æ®é›†ï¼Œä½¿ç”¨ç‰¹å®šçš„é…ç½®
                if alt_dataset == "Junwei/MSR":
                    try:
                        # Junwei/MSRå¯èƒ½éœ€è¦ç‰¹å®šçš„é…ç½®æˆ–å­é›†
                        full_dataset = self._load_dataset_with_retry(alt_dataset, max_retries=2)
                        if full_dataset is not None:
                            self.logger.info(f"æˆåŠŸåŠ è½½æ›¿ä»£æ•°æ®é›†: {alt_dataset}")
                            break
                    except Exception as e:
                        self.logger.warning(f"Junwei/MSRåŠ è½½å¤±è´¥: {e}, ç»§ç»­å°è¯•å…¶ä»–æ›¿ä»£æ•°æ®é›†")
                        continue
                else:
                    full_dataset = self._load_dataset_with_retry(alt_dataset, max_retries=2)
                    if full_dataset is not None:
                        self.logger.info(f"æˆåŠŸåŠ è½½æ›¿ä»£æ•°æ®é›†: {alt_dataset}")
                        break
            
            if full_dataset is None:
                self.logger.error("æ‰€æœ‰æ•°æ®é›†åŠ è½½å°è¯•å‡å¤±è´¥")
                return None
        
        try:
            # åˆ†å‰²æ•°æ®é›†
            total_size = len(full_dataset["train"])
            train_size = int(total_size * 0.8)
            val_size = int(total_size * 0.1)
            
            train_dataset = full_dataset["train"].select(range(train_size))
            val_dataset = full_dataset["train"].select(range(train_size, train_size + val_size))
            test_dataset = full_dataset["train"].select(range(train_size + val_size))
            
            return {
                "train": train_dataset,
                "test": test_dataset,
                "val": val_dataset
            }
        except Exception as e:
            self.logger.error(f"å¤„ç†BigVulæ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def _load_reveal_dataset(self) -> Optional[Dict[str, Dataset]]:
        """åŠ è½½Revealæ•°æ®é›† - å¢å¼ºç‰ˆç½‘ç»œå¤„ç†"""
        self.logger.info("åŠ è½½ Reveal æ•°æ®é›†...")
        
        # é¦–å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
        if not self._test_network_connectivity():
            self.logger.error("ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ— æ³•è®¿é—® HuggingFace Hub")
            return None
        
        # å°è¯•åŠ è½½ä¸»è¦æ•°æ®é›†
        full_dataset = self._load_dataset_with_retry("claudios/ReVeal")
        if full_dataset is None:
            self.logger.warning("ä¸»è¦æ•°æ®é›† claudios/ReVeal åŠ è½½å¤±è´¥ï¼Œå°è¯•æ›¿ä»£æ•°æ®é›†...")
            
            # å°è¯•æ›¿ä»£æ•°æ®é›†
            alternatives = self._get_available_alternatives("reveal")
            for alt_dataset in alternatives:
                self.logger.info(f"å°è¯•æ›¿ä»£æ•°æ®é›†: {alt_dataset}")
                full_dataset = self._load_dataset_with_retry(alt_dataset, max_retries=2)
                if full_dataset is not None:
                    self.logger.info(f"æˆåŠŸåŠ è½½æ›¿ä»£æ•°æ®é›†: {alt_dataset}")
                    break
            
            if full_dataset is None:
                self.logger.error("æ‰€æœ‰æ•°æ®é›†åŠ è½½å°è¯•å‡å¤±è´¥")
                return None
        
        try:
            # åˆ†å‰²æ•°æ®é›†
            total_size = len(full_dataset["train"])
            train_size = int(total_size * 0.8)
            val_size = int(total_size * 0.1)
            
            train_dataset = full_dataset["train"].select(range(train_size))
            val_dataset = full_dataset["train"].select(range(train_size, train_size + val_size))
            test_dataset = full_dataset["train"].select(range(train_size + val_size))
            
            return {
                "train": train_dataset,
                "test": test_dataset,
                "val": val_dataset
            }
        except Exception as e:
            self.logger.error(f"å¤„ç†Revealæ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def _load_devign_dataset(self) -> Optional[Dict[str, Dataset]]:
        """åŠ è½½Devignæ•°æ®é›† - å¢å¼ºç‰ˆç½‘ç»œå¤„ç†"""
        self.logger.info("åŠ è½½ Devign æ•°æ®é›†...")
        
        # é¦–å…ˆæµ‹è¯•ç½‘ç»œè¿æ¥
        if not self._test_network_connectivity():
            self.logger.error("ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ— æ³•è®¿é—® HuggingFace Hub")
            return None
        
        # å°è¯•åŠ è½½ä¸»è¦æ•°æ®é›†
        full_dataset = self._load_dataset_with_retry("DetectVul/devign")
        if full_dataset is None:
            self.logger.warning("ä¸»è¦æ•°æ®é›† DetectVul/devign åŠ è½½å¤±è´¥ï¼Œå°è¯•æ›¿ä»£æ•°æ®é›†...")
            
            # å°è¯•æ›¿ä»£æ•°æ®é›†
            alternatives = self._get_available_alternatives("devign")
            for alt_dataset in alternatives:
                self.logger.info(f"å°è¯•æ›¿ä»£æ•°æ®é›†: {alt_dataset}")
                full_dataset = self._load_dataset_with_retry(alt_dataset, max_retries=2)
                if full_dataset is not None:
                    self.logger.info(f"æˆåŠŸåŠ è½½æ›¿ä»£æ•°æ®é›†: {alt_dataset}")
                    break
            
            if full_dataset is None:
                self.logger.error("æ‰€æœ‰æ•°æ®é›†åŠ è½½å°è¯•å‡å¤±è´¥")
                return None
        
        try:
            # åˆ†å‰²æ•°æ®é›†
            total_size = len(full_dataset["train"])
            train_size = int(total_size * 0.8)
            val_size = int(total_size * 0.1)
            
            train_dataset = full_dataset["train"].select(range(train_size))
            val_dataset = full_dataset["train"].select(range(train_size, train_size + val_size))
            test_dataset = full_dataset["train"].select(range(train_size + val_size))
            
            return {
                "train": train_dataset,
                "test": test_dataset,
                "val": val_dataset
            }
        except Exception as e:
            self.logger.error(f"å¤„ç†Devignæ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def _save_dataset(self, dataset: Dict[str, Dataset], dataset_name: str) -> bool:
        """
        ä¿å­˜æ•°æ®é›†åˆ°æœ¬åœ°æ–‡ä»¶
        
        Args:
            dataset: æ•°æ®é›†å­—å…¸
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            local_files = self.config.datasets[dataset_name]["local_files"]
            
            for split, split_dataset in dataset.items():
                filename = local_files[split]
                filepath = self.data_root / filename
                
                self.logger.info(f"ä¿å­˜ {dataset_name}/{split} åˆ° {filepath}")
                
                # è½¬æ¢ä¸ºPandas DataFrameå¹¶ä¿å­˜ä¸ºJSON
                df = split_dataset.to_pandas()
                df.to_json(filepath, orient='records', lines=True, force_ascii=False)
                
                # éªŒè¯æ–‡ä»¶
                if filepath.exists():
                    file_size = filepath.stat().st_size
                    self.logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {filepath} ({file_size} bytes)")
                else:
                    self.logger.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {filepath}")
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def preprocess_datasets(self) -> bool:
        """
        é¢„å¤„ç†æ•°æ®é›†
        
        Returns:
            bool: é¢„å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("å¼€å§‹é¢„å¤„ç†æ•°æ®é›†...")
        
        try:
            for dataset_name in self.config.datasets.keys():
                self.logger.info(f"é¢„å¤„ç†æ•°æ®é›†: {dataset_name}")
                success = self._preprocess_single_dataset(dataset_name)
                if not success:
                    self.logger.error(f"æ•°æ®é›† {dataset_name} é¢„å¤„ç†å¤±è´¥")
                    return False
            
            self.logger.info("æ‰€æœ‰æ•°æ®é›†é¢„å¤„ç†å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def check_network_and_provide_solutions(self) -> Dict:
        """
        æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒHuggingFace Hubè®¿é—®ï¼Œå¹¶æä¾›è§£å†³æ–¹æ¡ˆï¼ˆæ”¯æŒé•œåƒæºï¼‰
        
        Returns:
            Dict: æ£€æŸ¥ç»“æœå’Œè§£å†³æ–¹æ¡ˆ
        """
        result = {
            "status": "",
            "hf_access": "",
            "mirrors_status": {},
            "solutions": []
        }
        
        # æµ‹è¯•æ‰€æœ‰é•œåƒæºçš„è¿æ¥çŠ¶æ€
        mirror_status = {}
        for mirror in self.hf_mirrors:
            try:
                response = requests.get(mirror, timeout=10)
                if response.status_code == 200:
                    mirror_status[mirror] = "å¯è®¿é—®"
                else:
                    mirror_status[mirror] = f"HTTP {response.status_code}"
            except Exception as e:
                mirror_status[mirror] = f"è¿æ¥å¤±è´¥: {str(e)}"
        
        result["mirrors_status"] = mirror_status
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„é•œåƒæº
        available_mirrors = [m for m, status in mirror_status.items() if status == "å¯è®¿é—®"]
        
        if available_mirrors:
            result["status"] = f"ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œ{len(available_mirrors)}ä¸ªé•œåƒæºå¯ç”¨"
            result["hf_access"] = f"HuggingFace Hubè®¿é—®æ­£å¸¸ (ä½¿ç”¨é•œåƒæº: {available_mirrors[0]})"
        else:
            result["status"] = "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ‰€æœ‰é•œåƒæºå‡ä¸å¯ç”¨"
            result["hf_access"] = "HuggingFace Hubè®¿é—®å¤±è´¥"
            
            result["solutions"].extend([
                "æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸",
                "é…ç½®ä»£ç†æœåŠ¡å™¨ (è®¾ç½®HTTP_PROXY/HTTPS_PROXYç¯å¢ƒå˜é‡)",
                "ä½¿ç”¨VPNè¿æ¥",
                "æ£€æŸ¥é˜²ç«å¢™è®¾ç½®",
                "æ£€æŸ¥DNSè®¾ç½®",
                "å°è¯•é‡å¯ç½‘ç»œè®¾å¤‡",
                "è”ç³»ç½‘ç»œç®¡ç†å‘˜",
                "ç¨åé‡è¯•"
            ])
        
        # æ·»åŠ é•œåƒæºé…ç½®å»ºè®®
        result["solutions"].extend([
            "å½“å‰é…ç½®çš„é•œåƒæº:" + ", ".join(self.hf_mirrors),
            "å¦‚éœ€æ·»åŠ æ›´å¤šé•œåƒæºï¼Œå¯ä¿®æ”¹HF_MIRRORSåˆ—è¡¨",
            "è®¾ç½®ç¯å¢ƒå˜é‡: export HF_ENDPOINT=https://hf-mirror.com",
            "è®¾ç½®ç¯å¢ƒå˜é‡: export HF_HUB_ENABLE_HF_TRANSFER=1"
        ])
        
        return result
    
    def _preprocess_single_dataset(self, dataset_name: str) -> bool:
        """
        é¢„å¤„ç†å•ä¸ªæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            bool: é¢„å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            local_files = self.config.datasets[dataset_name]["local_files"]
            processed_files = self.config.datasets[dataset_name]["processed_files"]
            
            processed_data = {}
            
            for split in ["train", "test"]:
                filename = local_files[split]
                filepath = self.data_root / filename
                
                if not filepath.exists():
                    self.logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                    continue
                
                # è¯»å–åŸå§‹æ•°æ®
                df = pd.read_json(filepath, orient='records', lines=True)
                
                # æ ‡å‡†åŒ–å­—æ®µå
                if dataset_name == "bigvul":
                    df = self._standardize_bigvul(df)
                elif dataset_name == "reveal":
                    df = self._standardize_reveal(df)
                elif dataset_name == "devign":
                    df = self._standardize_devign(df)
                
                processed_data[split] = df
                
                # ä¿å­˜å¤„ç†åçš„æ•°æ®
                processed_filename = processed_files[split]
                processed_filepath = self.data_root / processed_filename
                
                processed_df = df.to_json(processed_filepath, orient='records', lines=True, force_ascii=False)
                self.logger.info(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {processed_filepath}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _standardize_bigvul(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–BigVulæ•°æ®é›†æ ¼å¼"""
        # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        column_mapping = {
            'func_before': 'code',
            'func_after': 'code_fixed',
            'vul': 'label',
            'project': 'project',
            'CVE ID': 'cve_id'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df = df.rename(columns={old_col: new_col})
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'label' in df.columns:
            df['label'] = df['label'].astype(int)
        
        return df
    
    def _standardize_reveal(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–Revealæ•°æ®é›†æ ¼å¼"""
        # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        column_mapping = {
            'functionSource': 'code',
            'label': 'label',
            'project': 'project'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df = df.rename(columns={old_col: new_col})
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'label' in df.columns:
            df['label'] = df['label'].astype(int)
        
        return df
    
    def _standardize_devign(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–Devignæ•°æ®é›†æ ¼å¼"""
        # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        column_mapping = {
            'func': 'code',
            'target': 'label',
            'project': 'project'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df = df.rename(columns={old_col: new_col})
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'label' in df.columns:
            # Devignæ•°æ®é›†çš„æ ‡ç­¾å¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            try:
                # é¦–å…ˆå°è¯•ç›´æ¥è½¬æ¢ä¸ºæ•´æ•°
                df['label'] = df['label'].astype(int)
            except (ValueError, TypeError):
                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                self.logger.warning("Devignæ ‡ç­¾åˆ—åŒ…å«åˆ—è¡¨æ ¼å¼ï¼Œè¿›è¡Œç‰¹æ®Šå¤„ç†")
                df['label'] = df['label'].apply(lambda x: int(x[0]) if isinstance(x, list) and len(x) > 0 else int(x))
        
        return df
    
    def check_data_status(self) -> Dict[str, Dict[str, bool]]:
        """
        æ£€æŸ¥æ•°æ®ä¸‹è½½çŠ¶æ€
        
        Returns:
            Dict: æ•°æ®é›†çŠ¶æ€ä¿¡æ¯
        """
        status = {}
        
        for dataset_name, dataset_info in self.config.datasets.items():
            status[dataset_name] = {}
            
            # æ£€æŸ¥åŸå§‹æ–‡ä»¶
            for split, filename in dataset_info["local_files"].items():
                filepath = self.data_root / filename
                status[dataset_name][f"{split}_raw"] = filepath.exists()
            
            # æ£€æŸ¥å¤„ç†åæ–‡ä»¶
            for split, filename in dataset_info["processed_files"].items():
                filepath = self.data_root / filename
                status[dataset_name][f"{split}_processed"] = filepath.exists()
        
        return status
    
    def print_status(self):
        """æ‰“å°æ•°æ®çŠ¶æ€"""
        status = self.check_data_status()
        
        print("\n" + "="*60)
        print("æ•°æ®ä¸‹è½½çŠ¶æ€æ£€æŸ¥")
        print("="*60)
        
        for dataset_name, dataset_status in status.items():
            print(f"\nğŸ“Š {dataset_name.upper()} æ•°æ®é›†:")
            for file_type, exists in dataset_status.items():
                status_icon = "âœ…" if exists else "âŒ"
                file_size = ""
                
                if exists:
                    # è·å–æ–‡ä»¶å¤§å°
                    filename = self.config.datasets[dataset_name]["local_files"].get(file_type.replace("_raw", ""), 
                                     self.config.datasets[dataset_name]["processed_files"].get(file_type.replace("_processed", ""), ""))
                    if filename:
                        filepath = self.data_root / filename
                        if filepath.exists():
                            size_mb = filepath.stat().st_size / (1024 * 1024)
                            file_size = f" ({size_mb:.1f}MB)"
                
                print(f"  {status_icon} {file_type}: {file_size}")
        
        print("\n" + "="*60)
    
    def run_full_preparation(self, datasets: List[str] = None):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å‡†å¤‡æµç¨‹
        
        Args:
            datasets: è¦å‡†å¤‡çš„æ•°æ®é›†åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå‡†å¤‡æ‰€æœ‰æ•°æ®é›†
        """
        if datasets is None:
            datasets = list(self.config.datasets.keys())
        
        print(f"å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹ï¼Œæ•°æ®é›†: {datasets}")
        print(f"æ•°æ®æ ¹ç›®å½•: {self.data_root}")
        
        success_count = 0
        total_count = len(datasets)
        
        for dataset_name in datasets:
            print(f"\n{'='*20} å¤„ç† {dataset_name} {'='*20}")
            
            # ä¸‹è½½æ•°æ®é›†
            if self.download_dataset(dataset_name):
                success_count += 1
            else:
                print(f"âŒ {dataset_name} ä¸‹è½½å¤±è´¥")
        
        print(f"\n{'='*20} æ•°æ®ä¸‹è½½æ€»ç»“ {'='*20}")
        print(f"æˆåŠŸ: {success_count}/{total_count}")
        
        if success_count == total_count:
            print("âœ… æ‰€æœ‰æ•°æ®é›†ä¸‹è½½æˆåŠŸï¼Œå¼€å§‹é¢„å¤„ç†...")
            # é¢„å¤„ç†æ•°æ®é›†
            if self.preprocess_datasets():
                print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
            else:
                print("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
        else:
            print("âŒ éƒ¨åˆ†æ•°æ®é›†ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡é¢„å¤„ç†")
        
        # æ‰“å°æœ€ç»ˆçŠ¶æ€
        self.print_status()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="GRACEé¡¹ç›®æ•°æ®å‡†å¤‡å·¥å…·")
    parser.add_argument("--data-root", type=str, default="/root/sj-tmp/dataset/",
                       help="æ•°æ®å­˜å‚¨æ ¹ç›®å½•")
    parser.add_argument("--dataset", type=str, 
                       choices=["bigvul", "reveal", "devign"],
                       help="æŒ‡å®šè¦ä¸‹è½½çš„æ•°æ®é›†")
    parser.add_argument("--all", action="store_true",
                       help="ä¸‹è½½æ‰€æœ‰æ•°æ®é›†")
    parser.add_argument("--check", action="store_true",
                       help="æ£€æŸ¥æ•°æ®ä¸‹è½½çŠ¶æ€")
    parser.add_argument("--preprocess", action="store_true",
                       help="ä»…é¢„å¤„ç†å·²ä¸‹è½½çš„æ•°æ®")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ•°æ®å‡†å¤‡å™¨
    preparator = DataPreparator(data_root=args.data_root)
    
    if args.check:
        # æ£€æŸ¥çŠ¶æ€
        preparator.print_status()
        
    elif args.preprocess:
        # ä»…é¢„å¤„ç†
        if preparator.preprocess_datasets():
            print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        else:
            print("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
            
    elif args.dataset:
        # ä¸‹è½½æŒ‡å®šæ•°æ®é›†
        preparator.run_full_preparation([args.dataset])
        
    elif args.all:
        # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        preparator.run_full_preparation()
        
    else:
        # é»˜è®¤è¡Œä¸ºï¼šä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        print("æœªæŒ‡å®šæ“ä½œï¼Œé»˜è®¤ä¸‹è½½æ‰€æœ‰æ•°æ®é›†")
        preparator.run_full_preparation()

if __name__ == "__main__":
    main()