#!/usr/bin/env python3
"""
GRACEé¡¹ç›®æ•°æ®å‡†å¤‡è„šæœ¬
è‡ªåŠ¨ä»Hugging Faceä¸‹è½½å’Œå‡†å¤‡æ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    python prepare_data.py --data-root /root/sj-tmp/dataset/
    python prepare_data.py --download-model  # ä¸‹è½½æ•°æ®é›†
    python prepare_data.py --all  # ä¸‹è½½æ‰€æœ‰æ•°æ®
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

try:
    from datasets import load_dataset, Dataset
    from datasets.exceptions import DatasetNotFoundError
    import pandas as pd
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·è¿è¡Œ: pip install -r requirements.txt")
    sys.exit(1)

from config.config import Config

class DataPreparator:
    """æ•°æ®ä¸‹è½½å’Œå‡†å¤‡å™¨"""
    
    def __init__(self, data_root: str = "/root/sj-tmp/dataset/"):
        """
        åˆå§‹åŒ–æ•°æ®å‡†å¤‡å™¨
        
        Args:
            data_root: æ•°æ®é›†å­˜å‚¨æ ¹ç›®å½•
        """
        self.data_root = Path(data_root)
        self.config = Config(data_root=data_root)
        self.setup_logging()
        
        # ç¡®ä¿æ•°æ®æ ¹ç›®å½•å­˜åœ¨
        self.data_root.mkdir(parents=True, exist_ok=True)
        
        print(f"æ•°æ®æ ¹ç›®å½•: {self.data_root}")
        print(f"é…ç½®æ•°æ®é›†: {list(self.config.datasets.keys())}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = self.config.logs_dir
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"data_preparation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format=self.config.log_format,
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"æ•°æ®å‡†å¤‡æ—¥å¿—: {log_file}")
    
    def download_dataset(self, dataset_name: str) -> bool:
        """
        ä¸‹è½½æŒ‡å®šæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§° (bigvul, reveal, devign)
            
        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if dataset_name not in self.config.datasets:
            self.logger.error(f"æœªçŸ¥æ•°æ®é›†: {dataset_name}")
            return False
        
        dataset_info = self.config.datasets[dataset_name]
        huggingface_url = dataset_info["huggingface_url"]
        
        self.logger.info(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_name} ({huggingface_url})")
        
        try:
            # ä»Hugging FaceåŠ è½½æ•°æ®é›†
            if dataset_name == "bigvul":
                dataset = self._load_bigvul_dataset()
            elif dataset_name == "reveal":
                dataset = self._load_reveal_dataset()
            elif dataset_name == "devign":
                dataset = self._load_devign_dataset()
            else:
                self.logger.error(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
                return False
            
            if dataset is None:
                self.logger.error(f"æ•°æ®é›† {dataset_name} ä¸‹è½½å¤±è´¥")
                return False
            
            # ä¿å­˜æ•°æ®é›†
            success = self._save_dataset(dataset, dataset_name)
            
            if success:
                self.logger.info(f"æ•°æ®é›† {dataset_name} ä¸‹è½½å®Œæˆ")
                return True
            else:
                self.logger.error(f"æ•°æ®é›† {dataset_name} ä¿å­˜å¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"ä¸‹è½½æ•°æ®é›† {dataset_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _load_bigvul_dataset(self) -> Optional[Dict[str, Dataset]]:
        """åŠ è½½BigVulæ•°æ®é›†"""
        self.logger.info("åŠ è½½ BigVul æ•°æ®é›†...")
        try:
            # åŠ è½½è®­ç»ƒé›†
            train_dataset = load_dataset("bstee615/bigvul", split="train[:80%]")
            # åŠ è½½æµ‹è¯•é›†
            test_dataset = load_dataset("bstee615/bigvul", split="train[80%:]")
            # åˆ›å»ºéªŒè¯é›†ï¼ˆä»è®­ç»ƒé›†åˆ†å‡ºä¸€éƒ¨åˆ†ï¼‰
            val_dataset = load_dataset("bstee615/bigvul", split="train[:10%]")
            
            return {
                "train": train_dataset,
                "test": test_dataset,
                "val": val_dataset
            }
        except Exception as e:
            self.logger.error(f"åŠ è½½BigVulæ•°æ®é›†å¤±è´¥: {e}")
            return None
    
    def _load_reveal_dataset(self) -> Optional[Dict[str, Dataset]]:
        """åŠ è½½Revealæ•°æ®é›†"""
        self.logger.info("åŠ è½½ Reveal æ•°æ®é›†...")
        try:
            # åŠ è½½æ•°æ®é›†
            full_dataset = load_dataset("claudios/ReVeal")
            
            # åˆ†å‰²æ•°æ®é›†
            total_size = len(full_dataset["train"])
            train_size = int(total_size * 0.8)
            val_size = int(total_size * 0.1)
            
            train_dataset = full_dataset["train"].select(range(train_size))
            val_dataset = full_dataset["train"].select(range(train_size, train_size + val_size))
            test_dataset = full_dataset["train"].select(range(train_size + val_size))
            
            return {
                "train": train_dataset,
                "test": test_dataset,
                "val": val_dataset
            }
        except Exception as e:
            self.logger.error(f"åŠ è½½Revealæ•°æ®é›†å¤±è´¥: {e}")
            return None
    
    def _load_devign_dataset(self) -> Optional[Dict[str, Dataset]]:
        """åŠ è½½Devignæ•°æ®é›†"""
        self.logger.info("åŠ è½½ Devign æ•°æ®é›†...")
        try:
            # åŠ è½½æ•°æ®é›†
            full_dataset = load_dataset("DetectVul/devign")
            
            # åˆ†å‰²æ•°æ®é›†
            total_size = len(full_dataset["train"])
            train_size = int(total_size * 0.8)
            val_size = int(total_size * 0.1)
            
            train_dataset = full_dataset["train"].select(range(train_size))
            val_dataset = full_dataset["train"].select(range(train_size, train_size + val_size))
            test_dataset = full_dataset["train"].select(range(train_size + val_size))
            
            return {
                "train": train_dataset,
                "test": test_dataset,
                "val": val_dataset
            }
        except Exception as e:
            self.logger.error(f"åŠ è½½Devignæ•°æ®é›†å¤±è´¥: {e}")
            return None
    
    def _save_dataset(self, dataset: Dict[str, Dataset], dataset_name: str) -> bool:
        """
        ä¿å­˜æ•°æ®é›†åˆ°æœ¬åœ°æ–‡ä»¶
        
        Args:
            dataset: æ•°æ®é›†å­—å…¸
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            local_files = self.config.datasets[dataset_name]["local_files"]
            
            for split, split_dataset in dataset.items():
                filename = local_files[split]
                filepath = self.data_root / filename
                
                self.logger.info(f"ä¿å­˜ {dataset_name}/{split} åˆ° {filepath}")
                
                # è½¬æ¢ä¸ºPandas DataFrameå¹¶ä¿å­˜ä¸ºJSON
                df = split_dataset.to_pandas()
                df.to_json(filepath, orient='records', lines=True, force_ascii=False)
                
                # éªŒè¯æ–‡ä»¶
                if filepath.exists():
                    file_size = filepath.stat().st_size
                    self.logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {filepath} ({file_size} bytes)")
                else:
                    self.logger.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {filepath}")
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def preprocess_datasets(self) -> bool:
        """
        é¢„å¤„ç†æ•°æ®é›†
        
        Returns:
            bool: é¢„å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("å¼€å§‹é¢„å¤„ç†æ•°æ®é›†...")
        
        try:
            for dataset_name in self.config.datasets.keys():
                self.logger.info(f"é¢„å¤„ç†æ•°æ®é›†: {dataset_name}")
                success = self._preprocess_single_dataset(dataset_name)
                if not success:
                    self.logger.error(f"æ•°æ®é›† {dataset_name} é¢„å¤„ç†å¤±è´¥")
                    return False
            
            self.logger.info("æ‰€æœ‰æ•°æ®é›†é¢„å¤„ç†å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†æ•°æ®é›†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _preprocess_single_dataset(self, dataset_name: str) -> bool:
        """
        é¢„å¤„ç†å•ä¸ªæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            bool: é¢„å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            local_files = self.config.datasets[dataset_name]["local_files"]
            processed_files = self.config.datasets[dataset_name]["processed_files"]
            
            processed_data = {}
            
            for split in ["train", "test"]:
                filename = local_files[split]
                filepath = self.data_root / filename
                
                if not filepath.exists():
                    self.logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                    continue
                
                # è¯»å–åŸå§‹æ•°æ®
                df = pd.read_json(filepath, orient='records', lines=True)
                
                # æ ‡å‡†åŒ–å­—æ®µå
                if dataset_name == "bigvul":
                    df = self._standardize_bigvul(df)
                elif dataset_name == "reveal":
                    df = self._standardize_reveal(df)
                elif dataset_name == "devign":
                    df = self._standardize_devign(df)
                
                processed_data[split] = df
                
                # ä¿å­˜å¤„ç†åçš„æ•°æ®
                processed_filename = processed_files[split]
                processed_filepath = self.data_root / processed_filename
                
                processed_df = df.to_json(processed_filepath, orient='records', lines=True, force_ascii=False)
                self.logger.info(f"å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {processed_filepath}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"é¢„å¤„ç†æ•°æ®é›† {dataset_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False
    
    def _standardize_bigvul(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–BigVulæ•°æ®é›†æ ¼å¼"""
        # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        column_mapping = {
            'func_before': 'code',
            'func_after': 'code_fixed',
            'vul': 'label',
            'project': 'project',
            'CVE ID': 'cve_id'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df = df.rename(columns={old_col: new_col})
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'label' in df.columns:
            df['label'] = df['label'].astype(int)
        
        return df
    
    def _standardize_reveal(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–Revealæ•°æ®é›†æ ¼å¼"""
        # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        column_mapping = {
            'functionSource': 'code',
            'label': 'label',
            'project': 'project'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df = df.rename(columns={old_col: new_col})
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'label' in df.columns:
            df['label'] = df['label'].astype(int)
        
        return df
    
    def _standardize_devign(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–Devignæ•°æ®é›†æ ¼å¼"""
        # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
        column_mapping = {
            'func': 'code',
            'target': 'label',
            'project': 'project'
        }
        
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df = df.rename(columns={old_col: new_col})
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        if 'label' in df.columns:
            df['label'] = df['label'].astype(int)
        
        return df
    
    def check_data_status(self) -> Dict[str, Dict[str, bool]]:
        """
        æ£€æŸ¥æ•°æ®ä¸‹è½½çŠ¶æ€
        
        Returns:
            Dict: æ•°æ®é›†çŠ¶æ€ä¿¡æ¯
        """
        status = {}
        
        for dataset_name, dataset_info in self.config.datasets.items():
            status[dataset_name] = {}
            
            # æ£€æŸ¥åŸå§‹æ–‡ä»¶
            for split, filename in dataset_info["local_files"].items():
                filepath = self.data_root / filename
                status[dataset_name][f"{split}_raw"] = filepath.exists()
            
            # æ£€æŸ¥å¤„ç†åæ–‡ä»¶
            for split, filename in dataset_info["processed_files"].items():
                filepath = self.data_root / filename
                status[dataset_name][f"{split}_processed"] = filepath.exists()
        
        return status
    
    def print_status(self):
        """æ‰“å°æ•°æ®çŠ¶æ€"""
        status = self.check_data_status()
        
        print("\n" + "="*60)
        print("æ•°æ®ä¸‹è½½çŠ¶æ€æ£€æŸ¥")
        print("="*60)
        
        for dataset_name, dataset_status in status.items():
            print(f"\nğŸ“Š {dataset_name.upper()} æ•°æ®é›†:")
            for file_type, exists in dataset_status.items():
                status_icon = "âœ…" if exists else "âŒ"
                file_size = ""
                
                if exists:
                    # è·å–æ–‡ä»¶å¤§å°
                    filename = self.config.datasets[dataset_name]["local_files"].get(file_type.replace("_raw", ""), 
                                     self.config.datasets[dataset_name]["processed_files"].get(file_type.replace("_processed", ""), ""))
                    if filename:
                        filepath = self.data_root / filename
                        if filepath.exists():
                            size_mb = filepath.stat().st_size / (1024 * 1024)
                            file_size = f" ({size_mb:.1f}MB)"
                
                print(f"  {status_icon} {file_type}: {file_size}")
        
        print("\n" + "="*60)
    
    def run_full_preparation(self, datasets: List[str] = None):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å‡†å¤‡æµç¨‹
        
        Args:
            datasets: è¦å‡†å¤‡çš„æ•°æ®é›†åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºå‡†å¤‡æ‰€æœ‰æ•°æ®é›†
        """
        if datasets is None:
            datasets = list(self.config.datasets.keys())
        
        print(f"å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹ï¼Œæ•°æ®é›†: {datasets}")
        print(f"æ•°æ®æ ¹ç›®å½•: {self.data_root}")
        
        success_count = 0
        total_count = len(datasets)
        
        for dataset_name in datasets:
            print(f"\n{'='*20} å¤„ç† {dataset_name} {'='*20}")
            
            # ä¸‹è½½æ•°æ®é›†
            if self.download_dataset(dataset_name):
                success_count += 1
            else:
                print(f"âŒ {dataset_name} ä¸‹è½½å¤±è´¥")
        
        print(f"\n{'='*20} æ•°æ®ä¸‹è½½æ€»ç»“ {'='*20}")
        print(f"æˆåŠŸ: {success_count}/{total_count}")
        
        if success_count == total_count:
            print("âœ… æ‰€æœ‰æ•°æ®é›†ä¸‹è½½æˆåŠŸï¼Œå¼€å§‹é¢„å¤„ç†...")
            # é¢„å¤„ç†æ•°æ®é›†
            if self.preprocess_datasets():
                print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
            else:
                print("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
        else:
            print("âŒ éƒ¨åˆ†æ•°æ®é›†ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡é¢„å¤„ç†")
        
        # æ‰“å°æœ€ç»ˆçŠ¶æ€
        self.print_status()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="GRACEé¡¹ç›®æ•°æ®å‡†å¤‡å·¥å…·")
    parser.add_argument("--data-root", type=str, default="/root/sj-tmp/dataset/",
                       help="æ•°æ®å­˜å‚¨æ ¹ç›®å½•")
    parser.add_argument("--dataset", type=str, 
                       choices=["bigvul", "reveal", "devign"],
                       help="æŒ‡å®šè¦ä¸‹è½½çš„æ•°æ®é›†")
    parser.add_argument("--all", action="store_true",
                       help="ä¸‹è½½æ‰€æœ‰æ•°æ®é›†")
    parser.add_argument("--check", action="store_true",
                       help="æ£€æŸ¥æ•°æ®ä¸‹è½½çŠ¶æ€")
    parser.add_argument("--preprocess", action="store_true",
                       help="ä»…é¢„å¤„ç†å·²ä¸‹è½½çš„æ•°æ®")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ•°æ®å‡†å¤‡å™¨
    preparator = DataPreparator(data_root=args.data_root)
    
    if args.check:
        # æ£€æŸ¥çŠ¶æ€
        preparator.print_status()
        
    elif args.preprocess:
        # ä»…é¢„å¤„ç†
        if preparator.preprocess_datasets():
            print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        else:
            print("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥")
            
    elif args.dataset:
        # ä¸‹è½½æŒ‡å®šæ•°æ®é›†
        preparator.run_full_preparation([args.dataset])
        
    elif args.all:
        # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        preparator.run_full_preparation()
        
    else:
        # é»˜è®¤è¡Œä¸ºï¼šä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        print("æœªæŒ‡å®šæ“ä½œï¼Œé»˜è®¤ä¸‹è½½æ‰€æœ‰æ•°æ®é›†")
        preparator.run_full_preparation()

if __name__ == "__main__":
    main()